---
title: "On decision boundaries of tests-V2"
author: "Florian Priv√©"
date: "August 4, 2016"
output:                    # DO NOT CHANGE
  prettydoc::html_pretty:  # DO NOT CHANGE
    theme: cayman          # DO NOT CHANGE
    highlight: github      # DO NOT CHANGE
bibliography: bibliography.bib
---

In this post, I will talk about an alternative way to choose quantiles for tests, those you choose in order to have a 95% confidence interval (5% of type-I error). 
I will then show that this idea can be used to combine tests, with some illustrations in R.

## Example with the chi-squared distribution

Say that you have a test whose values under the null hypothesis ($H_0$) follow a chi-squared distribution with 10 degrees of freedom ($\chi_{10}^2$).

You can choose

* to reject $H_0$ only for the largest values of the statistic with significance $\alpha = 5\%$, which means rejecting the null hypothesis for values that are larger than the 95-percentile:

```{r, out.width=500, echo=FALSE, fig.align='center', fig.cap="One-tailed test"}
knitr::include_graphics("../images/post-tests/chi-squared_test2.jpg")
```

* or to reject $H_0$ for both largest and smallest values of the statistic. Indeed, smallest values could be considered "too good to be true" [@stuart1954]. Then, $H_0$ is rejected for values smaller than the 2.5-percentile or larger than the 97.5-percentile:

```{r, out.width=500, echo=FALSE, fig.align='center', fig.cap="Two-tailed test"}
knitr::include_graphics("../images/post-tests/chi-squared_test1.jpg")
```

__Why choosing? Why not letting the test choose by itself?__ 

What do I mean by this? If you make the boundary on the test statistic's density's values (y-axis), not on the statistic's values (x-axis), you always obtain a one-tailed test whatever is the distribution of the test statistic. You then reject all values that have a corresponding density lower than the 5-percentile. 

Let's see what this means in image:

```{r, out.width=500, echo=FALSE, fig.align='center', fig.cap="Always a one-tailed test, but with respect to the y-axis"}
knitr::include_graphics("../images/post-tests/chi-squared_test3.jpg")
```

I see this as __rejecting the 5\% less probable values__ ("probable" in terms of the test statistic's density).

## More convincing: application to the combination of tests

Combining tests may be a way to create a more powerful or robust test.

### First example: application in reliability

Say that you have two goodness-of-fit test statistic for the Weibull distribution (GOFW) (a well-known distribution in survival analysis). How to combine them? A priori, the best way I can see is to use their joint distribution. A 2D distribution has a density, as before, so we can find a threshold so that only 5% of this distribution's values have a density under this threshold. This threshold is also called a 95%-contour.

Again, an image will be clearer than words. I drew several samples of size 50 from the Weibull distribution and three alternatives to the Weibull distribution: the Gamma, Log-Normal and Dhillon I distributions. For all these samples, I computed the corresponding values of the two GOFWs, and I plotted these paired values:

```{r, out.width=600, echo=FALSE, fig.align='center'}
knitr::include_graphics("../images/post-tests/combi.jpg")
```

So, in black are the pair's values for several samples of the weibull distribution (the null hypothesis) and the alternatives are spread around. We have also in black the 95%-contour for $H_0$. So, points outside of this boundary correspond to samples for which we reject the null hypothesis. 

This gave one of the most powerful testd for the Weibull distribution. See the second example below for an example with R code.

### Second example: application in genomics

#### Introduction

Cochran-Armitage Trend Tests (CATT) is well used in genomics to test for association between a single marker and a disease [@Zheng2012, section 3.3.1]. When the true genetic model is respectively the REC, ADD,
or DOM model, the trend test ZCATT(x), where x = 0, x = 1/2, or x = 1 respectively, gives powerful tests. Yet, the true model is generally unknown and choosing one specific value of x can lead to poor powers for some alternative models. 

Then, the MAX3 statistic defined by $$MAX3 = \max\{|ZCATT(0)|, |ZCATT(1/2)|, |ZCATT(1)|\}$$ can be used to have a more robust test (the power of the test remains good whatever is the underlying model). Yet, we could make another robust test based on the idea of the previous section.

#### Simulation of values for these three statistics

I followed the algorithm detailed in [@Zheng2012, section 3.9.1] to simulate contingency tables under different parameters as, for example, the genotype relative risk (GRR) $\lambda_2$, the genetic model, the minor allele frequency (MAF) $p$, etc.

```{r}
source("D:/Projets/blog/code/simu_counts.R")
source("D:/Projets/blog/code/ZCATT.R")
```

Let us plot simulated values of three statistics, ZCATT(0), ZCATT(1/2) and ZCATT(1), by pairs. I will add to these plot two decision boundaries corresponding to the rejection of the null hypothesis for the statistics $MAX2 = \max\{|S_1|, |S_2|\}$ (the square) and $DENS2 = \hat{f}_{S_1, S_2}$ (the oval).

```{r, fig.align='center'}
source("D:/Projets/blog/code/square.R")
pacman::p_load(ks) 

LWD <- 3; PCH <- 19; CEX <- 0.5;
models <- c("REC", "ADD", "DOM")
n <- length(models)
ind2 <- c(0, 0.5, 1)
NSIM <- 200
lambda2 <- c(1.5, 1/1.5)
p <- 0.3

for (ind in -(1:3)) {
  counts <- simu_counts(nsim = 1e5, p = p)             
  simus <- sapply(ind2, function(x) ZCATT(counts, x = x))
  colnames(simus) <- paste0("ZCATT(", ind2, ")")
  simus.save <- replace(simus, is.na(simus), 0)[, ind]
  k <- ks::kde(x = simus.save)
  plot(simus.save[1:NSIM, ], cex = CEX, pch = PCH, lwd = LWD, 
       xlim = c(-3, 3), ylim = c(-3, 3)) 
  leg = c("NULL", models)
  legend(x = "topleft", legend = leg, pch = PCH, col = 1:(n+1))
  plot(k, cont = 95, add = TRUE, col = 5, lwd = LWD)
  q <- quantile(apply(abs(simus.save), 1, max), 0.95) 
  square(q, col = 5, lwd = LWD)
  for (lam2 in lambda2) {
    for (i in 1:n) {
      counts <- simu_counts(nsim = NSIM, model = models[i], 
                            lam2 = lam2, p = p)
      simus <- sapply(ind2, function(x) ZCATT(counts, x = x))
      simus <- replace(simus, is.na(simus), 0)[, ind]
      points(simus, col = i+1, cex = CEX, pch = PCH) 
    }
  }
}
```

Let us plot these three statistics' values in 3D:

```{r, collapse=FALSE}
pacman::p_load(rgl, rglwidget) 

counts <- simu_counts(nsim = NSIM, p = p)
simus <- sapply(c(0, 0.5, 1), function(x) ZCATT(counts, x = x))
rgl::plot3d(x = simus, size = 5, xlab = "ZCATT(0)",
            ylab = "ZCATT(1/2)", zlab = "ZCATT(1)")
for (lam2 in lambda2) {
  for (i in 1:length(models)) {
    counts <- simu_counts(nsim = NSIM, p = p, model = models[i], lam2 = lam2)
    simus <- sapply(c(0, 0.5, 1), function(x) ZCATT(counts, x = x))
    rgl::plot3d(x = simus, col = i+1, add = TRUE)
  }
}
rglwidget()
```

We can see the three statistics' values for the different models (H0: black, REC: red, ADD: green, DOM: blue) are almost on a same plane. So, it would be meaningless to estimate the 3D density. 

### Comparaison of MAX3 and this new test

I tried to project these simulated values in 2D through a Principal Component Analysis and then to make the new statistic with the 2D density of the first PCs.
It did give a robust test, yet slightly less powerful than MAX3.

The problem, I think, is that the 2D or 3D distribution of the statistics' values for the alternatives are highly correlated with the ones of the null hypothesis. So, even if the "area of H0" is larger with MAX2 than DENS2 (see 2D plots), the number of alternatives in there is lower, so gives a more powerful test.

## Conlusion

We have seen how to use density to get robust and powerful tests, without any subjective choice. We have also visualized some combination of tests and directly assessed from plots the powers of tests for multiple alternatives.

In practive, this works well with approximately normally distributed statistics because it's then easy to get a non-parametric estimation of the density via the use of a Gaussian Kernel (what does `kde`).

## References
