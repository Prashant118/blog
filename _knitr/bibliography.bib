@article{stuart1954,
 ISSN = {00359254, 14679876},
 URL = {http://www.jstor.org/stable/2985442},
 abstract = {In using the goodness-of-fit test should a very small value of Ï‡2 occasion a rejection of the null hypothesis? Mr Stuart presents the conflicting views on this question and answers it: No.},
 author = {Alan Stuart},
 journal = {Journal of the Royal Statistical Society. Series C (Applied Statistics)},
 number = {1},
 pages = {29-32},
 publisher = {[Wiley, Royal Statistical Society]},
 title = {Too Good to be True?},
 volume = {3},
 year = {1954}
}


@book{Zheng2012,
address = {Boston, MA},
author = {Zheng, Gang and Yang, Yaning and Zhu, Xiaofeng and Elston, Robert C.},
doi = {10.1007/978-1-4614-2245-7},
isbn = {978-1-4614-2244-0},
publisher = {Springer US},
series = {Statistics for Biology and Health},
title = {{Analysis of Genetic Association Studies}},
url = {http://link.springer.com/10.1007/978-1-4614-2245-7},
year = {2012}
}

@article{Kane2013,
abstract = {This paper presents two complementary statistical computing frameworks that address challenges in parallel processing and the analysis of massive data. First, the foreach package allows users of the R programming environment to define parallel loops that may be run sequentially on a single machine, in parallel on a symmetric multiprocessing (SMP) machine, or in cluster environments without platformspecific code. Second, the bigmemory package implements memoryand filemapped data structures that provide (a) access to arbitrarily large data while retaining a look and feel that is familiar to R users and (b) data structures that are shared across processor cores in order to support efficient parallel computing techniques. Although these packages may be used independently, this paper shows how they can be used in combination to address challenges that have effectively been beyond the reach of researchers who lack specialized software development skills or expensive hardware.},
author = {Kane, Michael J and Emerson, John W and Weston, Stephen},
doi = {10.18637/jss.v055.i14},
file = {:home/privef/Bureau/thesis-celiac/articles/Kane, Emerson, Weston - 2013 - Scalable Strategies for Computing with Massive Data.pdf:pdf},
issn = {15487660},
journal = {Journal of Statistical Software},
number = {14},
pages = {1--19},
title = {{Scalable Strategies for Computing with Massive Data}},
url = {http://www.jstatsoft.org/v55/i14/},
volume = {55},
year = {2013}
}

@article{Wickham2011,
abstract = {Many data analysis problems involve the application of a split-apply-combine strategy, where you break up a big problem into manageable pieces, operate on each piece inde- pendently and then put all the pieces back together. This insight gives rise to a new R package that allows you to smoothly apply this strategy, without having to worry about the type of structure in which your data is stored. The paper includes two case studies showing how these insights make it easier to work with batting records for veteran baseball players and a large 3d array of spatio-temporal ozone measurements. Keywords:},
author = {Wickham, Hadley},
doi = {10.1039/np9971400083},
issn = {0265-0568, 1460-4752},
journal = {Journal of Statistical Software},
keywords = {apply,data analysis,r,split},
number = {1},
pages = {1--29},
title = {{The Split-Apply-Combine Strategy for Data Analysis. Journal of Statistical Software}},
url = {http://www.jstatsoft.org/v40/i01/},
volume = {40},
year = {2011}
}